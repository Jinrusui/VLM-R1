{
  "os":  "Linux-5.15.0-122-generic-x86_64-with-glibc2.35",
  "python":  "3.10.16",
  "startedAt":  "2025-03-25T22:36:23.529797Z",
  "args":  [
    "examples/train_lora/qwen2vl_sft_config.yaml"
  ],
  "program":  "/opt/conda/envs/vlm-r1/bin/llamafactory-cli",
  "git":  {
    "remote":  "https://github.com/hiyouga/LLaMA-Factory.git",
    "commit":  "59e12bffe8d823eaa2ece2d0104379865e60a58d"
  },
  "email":  "sjr116757@gmail.com",
  "root":  "/jinru/VLM-R1/LLaMA-Factory",
  "host":  "cf88f124041e",
  "username":  "root",
  "executable":  "/opt/conda/envs/vlm-r1/bin/python3.10",
  "cpu_count":  64,
  "cpu_count_logical":  128,
  "gpu":  "[NVIDIA A100-SXM4-80GB]",
  "gpu_count":  1,
  "disk":  {
    "/":  {
      "total":  "56908316672",
      "used":  "38426365952"
    }
  },
  "memory":  {
    "total":  "540598202368"
  },
  "cpu":  {
    "count":  64,
    "countLogical":  128
  },
  "gpu_nvidia":  [
    {
      "name":  "NVIDIA A100-SXM4-80GB",
      "memoryTotal":  "85899345920",
      "cudaCores":  6912,
      "architecture":  "Ampere"
    }
  ],
  "cudaVersion":  "12.5"
}